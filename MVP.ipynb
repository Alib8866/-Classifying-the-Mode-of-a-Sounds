{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJOyCFhUId1QvWpjbFQdwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alib8866/-Classifying-the-Mode-of-a-Sounds/blob/main/MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "metadata": {
        "id": "XiY_MM3ok4RJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "Gy7uvLOJk8Tv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_train = pd.read_csv(\"/kaggle/input/network-intrusion-detection/Train_data.csv\")\n",
        "\n",
        "print(df_train.info())"
      ],
      "metadata": {
        "id": "CGdsieKAlIKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.get_dummies(df_train,columns = [\"protocol_type\", \"flag\", \"service\", \"class\"], drop_first=True)\n",
        "\n",
        "data_train.head()"
      ],
      "metadata": {
        "id": "yVBkZxTTlrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = data_train.class_normal.values\n",
        "x_data = data_train.drop([\"class_normal\"], axis=1)\n",
        "y_data = y_data.astype('int64') #we convert dtype of y_data to int64 here for a positive decrease in cost values.\n",
        "\n",
        "print(\"Y datas : \", y_data) #if it is normal 1 else 0.\n",
        "print(\"X datas : \\n\", x_data.head())"
      ],
      "metadata": {
        "id": "DHv_YTr9l2AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n",
        "\n",
        "x.head(n=10"
      ],
      "metadata": {
        "id": "UH2U0bWXl7Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[['num_outbound_cmds','is_host_login']].head()"
      ],
      "metadata": {
        "id": "2yd0GLZil_Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.drop([\"num_outbound_cmds\", \"is_host_login\"], axis = 1, inplace=True)\n",
        "x.head()"
      ],
      "metadata": {
        "id": "pAwEjtRdmJpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = x_train.T\n",
        "x_test = x_test.T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "\n",
        "print(\"x_train :\", x_train.shape)\n",
        "print(\"x_test :\", x_test.shape)\n",
        "print(\"y_train :\", y_train.shape)\n",
        "print(\"y_test :\", y_test.shape)"
      ],
      "metadata": {
        "id": "1ROPbWPRmP7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weigths_and_bias(dimension):\n",
        "    w = np.full((dimension,1),0.01)\n",
        "    b = 0.0\n",
        "    return w,b\n",
        "\n",
        "def sigmoid(z):\n",
        "    y_head = 1/(1 + np.exp(-z))\n",
        "    return y_head"
      ],
      "metadata": {
        "id": "jW9jdBk1mpbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_propagation(w,b,x_train,y_train):\n",
        "    #forward propagation\n",
        "    z = np.dot(w.T, x_train) + b\n",
        "    y_head = sigmoid(z)\n",
        "    loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head)\n",
        "    cost = (np.sum(loss)) / x_train.shape[1]\n",
        "    \n",
        "    #backward propagation \n",
        "    derivative_weight = (np.dot(x_train,((y_head-y_train).T))) / x_train.shape[1]    \n",
        "    derivative_bias = np.sum(y_head-y_train) / x_train.shape[1]    \n",
        "    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n",
        "    return cost, gradients"
      ],
      "metadata": {
        "id": "liXUDa_QmuWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n",
        "    cost_list = []\n",
        "    cost_list2 = []\n",
        "    index = []\n",
        "    \n",
        "    #updating(learning) parametersis number_of_iteration times \n",
        "    for i in range(number_of_iteration):\n",
        "        #make forward and backwar propagation to find cost and gradients\n",
        "        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n",
        "        cost_list.append(cost)\n",
        "        #lets update \n",
        "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
        "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
        "        if i % 3 == 0 :\n",
        "            cost_list2.append(cost)\n",
        "            index.append(i)\n",
        "            print(\"cost after iteration %i : %f\" %(i,cost))\n",
        "            \n",
        "    #store updated parameters in a parameters dictionary\n",
        "    parameters = {\"weight\":w,\"bias\":b}\n",
        "    plt.plot(index,cost_list2)\n",
        "    plt.xticks(index,rotation='vertical')\n",
        "    plt.xlabel(\"Number of iteration\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "    return parameters, gradients, cost_list"
      ],
      "metadata": {
        "id": "QmBZHHy7mzOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w,b,x_test):  \n",
        "    z = sigmoid(np.dot(w.T,x_test)+b) \n",
        "    Y_prediction = np.zeros((1,x_test.shape[1])) # the length of Y-prediction should be as the number of samples.\n",
        "    # if y_head is less than 0.5 the class is 0, else 1.\n",
        "    for i in range(z.shape[1]):\n",
        "        if z[0,i] <= 0.5:\n",
        "            Y_prediction[0,i] = 0\n",
        "        else:\n",
        "            Y_prediction[0,i] = 1\n",
        "        \n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "cxDKj5j-nEHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n",
        "    dimension = x_train.shape[0]\n",
        "    w,b = initialize_weigths_and_bias(dimension)\n",
        "    \n",
        "    #run learning algorithm\n",
        "    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n",
        "    \n",
        "    #We get prediction values with the test data and the trained parameters we give.\n",
        "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"], x_test) \n",
        "    \n",
        "    error_value = np.mean(np.abs(y_prediction_test - y_test)) * 100    \n",
        "    print(\"test accuracy: {} %\".format(100 - error_value ))\n",
        "    \n",
        "    return  y_prediction_test, y_test"
      ],
      "metadata": {
        "id": "M2t74r00nIds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prediction_test, y_test = logistic_regression(x_train, y_train, x_test, y_test, learning_rate=3, num_iterations=100)"
      ],
      "metadata": {
        "id": "mgb0pAwnnNKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_values = np.zeros((1,x_test.shape[1]))\n",
        "y_test_values[:1,:10] = y_test[:10] \n",
        "\n",
        "print(\"real : \", y_test_values[:1,:10])\n",
        "print(\"pred : \", y_prediction_test[:1,:10])"
      ],
      "metadata": {
        "id": "hvrxJVDYnaUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "lr.fit(x_train.T,y_train.T)\n",
        "print(\"test acuracy : {}\".format(lr.score(x_test.T,y_test.T)))"
      ],
      "metadata": {
        "id": "jtVzsod4nhc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}